<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="author" content="Ietje Penninga" />

<meta name="date" content="2015-07-23" />

<title>Practical Machine Learning</title>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; }
code > span.dt { color: #902000; }
code > span.dv { color: #40a070; }
code > span.bn { color: #40a070; }
code > span.fl { color: #40a070; }
code > span.ch { color: #4070a0; }
code > span.st { color: #4070a0; }
code > span.co { color: #60a0b0; font-style: italic; }
code > span.ot { color: #007020; }
code > span.al { color: #ff0000; font-weight: bold; }
code > span.fu { color: #06287e; }
code > span.er { color: #ff0000; font-weight: bold; }
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<link href="data:text/css,body%20%7B%0A%20%20background%2Dcolor%3A%20%23fff%3B%0A%20%20margin%3A%201em%20auto%3B%0A%20%20max%2Dwidth%3A%20700px%3B%0A%20%20overflow%3A%20visible%3B%0A%20%20padding%2Dleft%3A%202em%3B%0A%20%20padding%2Dright%3A%202em%3B%0A%20%20font%2Dfamily%3A%20%22Open%20Sans%22%2C%20%22Helvetica%20Neue%22%2C%20Helvetica%2C%20Arial%2C%20sans%2Dserif%3B%0A%20%20font%2Dsize%3A%2014px%3B%0A%20%20line%2Dheight%3A%201%2E35%3B%0A%7D%0A%0A%23header%20%7B%0A%20%20text%2Dalign%3A%20center%3B%0A%7D%0A%0A%23TOC%20%7B%0A%20%20clear%3A%20both%3B%0A%20%20margin%3A%200%200%2010px%2010px%3B%0A%20%20padding%3A%204px%3B%0A%20%20width%3A%20400px%3B%0A%20%20border%3A%201px%20solid%20%23CCCCCC%3B%0A%20%20border%2Dradius%3A%205px%3B%0A%0A%20%20background%2Dcolor%3A%20%23f6f6f6%3B%0A%20%20font%2Dsize%3A%2013px%3B%0A%20%20line%2Dheight%3A%201%2E3%3B%0A%7D%0A%20%20%23TOC%20%2Etoctitle%20%7B%0A%20%20%20%20font%2Dweight%3A%20bold%3B%0A%20%20%20%20font%2Dsize%3A%2015px%3B%0A%20%20%20%20margin%2Dleft%3A%205px%3B%0A%20%20%7D%0A%0A%20%20%23TOC%20ul%20%7B%0A%20%20%20%20padding%2Dleft%3A%2040px%3B%0A%20%20%20%20margin%2Dleft%3A%20%2D1%2E5em%3B%0A%20%20%20%20margin%2Dtop%3A%205px%3B%0A%20%20%20%20margin%2Dbottom%3A%205px%3B%0A%20%20%7D%0A%20%20%23TOC%20ul%20ul%20%7B%0A%20%20%20%20margin%2Dleft%3A%20%2D2em%3B%0A%20%20%7D%0A%20%20%23TOC%20li%20%7B%0A%20%20%20%20line%2Dheight%3A%2016px%3B%0A%20%20%7D%0A%0Atable%20%7B%0A%20%20margin%3A%201em%20auto%3B%0A%20%20border%2Dwidth%3A%201px%3B%0A%20%20border%2Dcolor%3A%20%23DDDDDD%3B%0A%20%20border%2Dstyle%3A%20outset%3B%0A%20%20border%2Dcollapse%3A%20collapse%3B%0A%7D%0Atable%20th%20%7B%0A%20%20border%2Dwidth%3A%202px%3B%0A%20%20padding%3A%205px%3B%0A%20%20border%2Dstyle%3A%20inset%3B%0A%7D%0Atable%20td%20%7B%0A%20%20border%2Dwidth%3A%201px%3B%0A%20%20border%2Dstyle%3A%20inset%3B%0A%20%20line%2Dheight%3A%2018px%3B%0A%20%20padding%3A%205px%205px%3B%0A%7D%0Atable%2C%20table%20th%2C%20table%20td%20%7B%0A%20%20border%2Dleft%2Dstyle%3A%20none%3B%0A%20%20border%2Dright%2Dstyle%3A%20none%3B%0A%7D%0Atable%20thead%2C%20table%20tr%2Eeven%20%7B%0A%20%20background%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0A%0Ap%20%7B%0A%20%20margin%3A%200%2E5em%200%3B%0A%7D%0A%0Ablockquote%20%7B%0A%20%20background%2Dcolor%3A%20%23f6f6f6%3B%0A%20%20padding%3A%200%2E25em%200%2E75em%3B%0A%7D%0A%0Ahr%20%7B%0A%20%20border%2Dstyle%3A%20solid%3B%0A%20%20border%3A%20none%3B%0A%20%20border%2Dtop%3A%201px%20solid%20%23777%3B%0A%20%20margin%3A%2028px%200%3B%0A%7D%0A%0Adl%20%7B%0A%20%20margin%2Dleft%3A%200%3B%0A%7D%0A%20%20dl%20dd%20%7B%0A%20%20%20%20margin%2Dbottom%3A%2013px%3B%0A%20%20%20%20margin%2Dleft%3A%2013px%3B%0A%20%20%7D%0A%20%20dl%20dt%20%7B%0A%20%20%20%20font%2Dweight%3A%20bold%3B%0A%20%20%7D%0A%0Aul%20%7B%0A%20%20margin%2Dtop%3A%200%3B%0A%7D%0A%20%20ul%20li%20%7B%0A%20%20%20%20list%2Dstyle%3A%20circle%20outside%3B%0A%20%20%7D%0A%20%20ul%20ul%20%7B%0A%20%20%20%20margin%2Dbottom%3A%200%3B%0A%20%20%7D%0A%0Apre%2C%20code%20%7B%0A%20%20background%2Dcolor%3A%20%23f7f7f7%3B%0A%20%20border%2Dradius%3A%203px%3B%0A%20%20color%3A%20%23333%3B%0A%7D%0Apre%20%7B%0A%20%20white%2Dspace%3A%20pre%2Dwrap%3B%20%20%20%20%2F%2A%20Wrap%20long%20lines%20%2A%2F%0A%20%20border%2Dradius%3A%203px%3B%0A%20%20margin%3A%205px%200px%2010px%200px%3B%0A%20%20padding%3A%2010px%3B%0A%7D%0Apre%3Anot%28%5Bclass%5D%29%20%7B%0A%20%20background%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0A%0Acode%20%7B%0A%20%20font%2Dfamily%3A%20Consolas%2C%20Monaco%2C%20%27Courier%20New%27%2C%20monospace%3B%0A%20%20font%2Dsize%3A%2085%25%3B%0A%7D%0Ap%20%3E%20code%2C%20li%20%3E%20code%20%7B%0A%20%20padding%3A%202px%200px%3B%0A%7D%0A%0Adiv%2Efigure%20%7B%0A%20%20text%2Dalign%3A%20center%3B%0A%7D%0Aimg%20%7B%0A%20%20background%2Dcolor%3A%20%23FFFFFF%3B%0A%20%20padding%3A%202px%3B%0A%20%20border%3A%201px%20solid%20%23DDDDDD%3B%0A%20%20border%2Dradius%3A%203px%3B%0A%20%20border%3A%201px%20solid%20%23CCCCCC%3B%0A%20%20margin%3A%200%205px%3B%0A%7D%0A%0Ah1%20%7B%0A%20%20margin%2Dtop%3A%200%3B%0A%20%20font%2Dsize%3A%2035px%3B%0A%20%20line%2Dheight%3A%2040px%3B%0A%7D%0A%0Ah2%20%7B%0A%20%20border%2Dbottom%3A%204px%20solid%20%23f7f7f7%3B%0A%20%20padding%2Dtop%3A%2010px%3B%0A%20%20padding%2Dbottom%3A%202px%3B%0A%20%20font%2Dsize%3A%20145%25%3B%0A%7D%0A%0Ah3%20%7B%0A%20%20border%2Dbottom%3A%202px%20solid%20%23f7f7f7%3B%0A%20%20padding%2Dtop%3A%2010px%3B%0A%20%20font%2Dsize%3A%20120%25%3B%0A%7D%0A%0Ah4%20%7B%0A%20%20border%2Dbottom%3A%201px%20solid%20%23f7f7f7%3B%0A%20%20margin%2Dleft%3A%208px%3B%0A%20%20font%2Dsize%3A%20105%25%3B%0A%7D%0A%0Ah5%2C%20h6%20%7B%0A%20%20border%2Dbottom%3A%201px%20solid%20%23ccc%3B%0A%20%20font%2Dsize%3A%20105%25%3B%0A%7D%0A%0Aa%20%7B%0A%20%20color%3A%20%230033dd%3B%0A%20%20text%2Ddecoration%3A%20none%3B%0A%7D%0A%20%20a%3Ahover%20%7B%0A%20%20%20%20color%3A%20%236666ff%3B%20%7D%0A%20%20a%3Avisited%20%7B%0A%20%20%20%20color%3A%20%23800080%3B%20%7D%0A%20%20a%3Avisited%3Ahover%20%7B%0A%20%20%20%20color%3A%20%23BB00BB%3B%20%7D%0A%20%20a%5Bhref%5E%3D%22http%3A%22%5D%20%7B%0A%20%20%20%20text%2Ddecoration%3A%20underline%3B%20%7D%0A%20%20a%5Bhref%5E%3D%22https%3A%22%5D%20%7B%0A%20%20%20%20text%2Ddecoration%3A%20underline%3B%20%7D%0A%0A%2F%2A%20Class%20described%20in%20https%3A%2F%2Fbenjeffrey%2Ecom%2Fposts%2Fpandoc%2Dsyntax%2Dhighlighting%2Dcss%0A%20%20%20Colours%20from%20https%3A%2F%2Fgist%2Egithub%2Ecom%2Frobsimmons%2F1172277%20%2A%2F%0A%0Acode%20%3E%20span%2Ekw%20%7B%20color%3A%20%23555%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%2F%2A%20Keyword%20%2A%2F%0Acode%20%3E%20span%2Edt%20%7B%20color%3A%20%23902000%3B%20%7D%20%2F%2A%20DataType%20%2A%2F%0Acode%20%3E%20span%2Edv%20%7B%20color%3A%20%2340a070%3B%20%7D%20%2F%2A%20DecVal%20%28decimal%20values%29%20%2A%2F%0Acode%20%3E%20span%2Ebn%20%7B%20color%3A%20%23d14%3B%20%7D%20%2F%2A%20BaseN%20%2A%2F%0Acode%20%3E%20span%2Efl%20%7B%20color%3A%20%23d14%3B%20%7D%20%2F%2A%20Float%20%2A%2F%0Acode%20%3E%20span%2Ech%20%7B%20color%3A%20%23d14%3B%20%7D%20%2F%2A%20Char%20%2A%2F%0Acode%20%3E%20span%2Est%20%7B%20color%3A%20%23d14%3B%20%7D%20%2F%2A%20String%20%2A%2F%0Acode%20%3E%20span%2Eco%20%7B%20color%3A%20%23888888%3B%20font%2Dstyle%3A%20italic%3B%20%7D%20%2F%2A%20Comment%20%2A%2F%0Acode%20%3E%20span%2Eot%20%7B%20color%3A%20%23007020%3B%20%7D%20%2F%2A%20OtherToken%20%2A%2F%0Acode%20%3E%20span%2Eal%20%7B%20color%3A%20%23ff0000%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%2F%2A%20AlertToken%20%2A%2F%0Acode%20%3E%20span%2Efu%20%7B%20color%3A%20%23900%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%2F%2A%20Function%20calls%20%2A%2F%20%0Acode%20%3E%20span%2Eer%20%7B%20color%3A%20%23a61717%3B%20background%2Dcolor%3A%20%23e3d2d2%3B%20%7D%20%2F%2A%20ErrorTok%20%2A%2F%0A%0A" rel="stylesheet" type="text/css" />

</head>

<body>



<div id="header">
<h1 class="title">Practical Machine Learning</h1>
<h4 class="author"><em>Ietje Penninga</em></h4>
<h4 class="date"><em>2015-07-23</em></h4>
</div>


<p>For Machine Learning class a dataset was obtained by: <code>http://groupware.les.inf.puc-rio.br/har#ixzz3gbvAGgXX</code>.</p>
<p>Reference: Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human ’13) . Stuttgart, Germany: ACM SIGCHI, 2013.</p>
<pre class="sourceCode r"><code class="sourceCode r">{training&lt;-<span class="kw">read.csv</span>(<span class="st">&quot;pml-training.csv&quot;</span>)
}</code></pre>
<p>Or download by calling: training&lt;-read.csv(url(“<a href="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv" class="uri">https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv</a>”))</p>
<p>The data contains a set of measurements on 6 atlethes that perform a series of weight lifting exercises. The way they perform the exercise is classified by an expert as A (= correct) , or as B:E (=various errors). Task is to determine based on the variables measured how the exercise is executed: A,B,C,D or E.</p>
<p>The training set consists of a number of measurements and a classification. Upon first analysis of the data it appears that the first 6 columns contain information about the atlethe and time and date of the measeurement. Of the other variables, there are some variables that generate a NA measurement in about 90% of the cases.I checked if the NA’s can be used to predict class A through E. But as you can see in this table the measurements with NA ’s (TRUE) are distributed equally among the classes.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">table</span>(<span class="kw">is.na</span>(training$stddev_yaw_forearm),training$classe)</code></pre>
<pre><code>##        
##            A    B    C    D    E
##   FALSE  109   79   70   69   79
##   TRUE  5471 3718 3352 3147 3528</code></pre>
<p>The variables with 90% NA’s were therefore considered unsuitable to build the model on, and were not used in subsequent modelling.</p>
<pre class="sourceCode r"><code class="sourceCode r"> {a&lt;-<span class="kw">is.na</span>(training); b&lt;-<span class="kw">colSums</span>(a)
 trainingCleanNA&lt;-training[,b&lt;<span class="dv">1</span>]
 }</code></pre>
<p>After this reduction of variables an analysis was performed on the variability of the variables using function nearZeroVar(). The variables with a near zero variability were dropped.</p>
<pre class="sourceCode r"><code class="sourceCode r">{<span class="kw">library</span>(caret)
nsv&lt;-<span class="kw">nearZeroVar</span>(trainingCleanNA,<span class="dt">saveMetrics=</span><span class="ot">TRUE</span>)
trainingCleanNAnzv&lt;-trainingCleanNA[,nsv$nzv==<span class="ot">FALSE</span>]
}</code></pre>
<pre><code>## Loading required package: lattice
## Loading required package: ggplot2</code></pre>
<p>Then a Random Forest calculation was performed on all remaining variables.</p>
<div id="cross-validation-error" class="section level2">
<h2>Cross validation error</h2>
<p>Random Forest algorithm estimates the error adequately, by calculating the Out Of Bag error(OOB). The OOB can be seen to converge after about 35 trees. Therefore the RandomForest fit was set to use 35 trees.</p>
<p>OOB error was estimated to well below 1 %</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(randomForest)</code></pre>
<pre><code>## randomForest 4.6-10
## Type rfNews() to see new features/changes/bug fixes.</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">modFitRfALLNZV&lt;-<span class="kw">randomForest</span>(trainingCleanNAnzv[,<span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">7</span>:<span class="dv">58</span>)],trainingCleanNAnzv[,<span class="dv">59</span>],<span class="dt">ntree=</span><span class="dv">35</span>,<span class="dt">do.trace=</span><span class="ot">TRUE</span>)</code></pre>
<pre><code>## ntree      OOB      1      2      3      4      5
##     1:   7.31%  4.00%  8.61% 10.56%  9.78%  5.80%
##     2:   7.73%  4.19%  9.62% 10.80% 10.06%  6.25%
##     3:   7.13%  4.19%  9.70%  9.25%  8.99%  5.31%
##     4:   6.63%  3.82%  9.47%  9.17%  7.48%  4.86%
##     5:   6.03%  3.32%  8.89%  8.53%  6.78%  4.14%
##     6:   5.42%  2.76%  7.68%  7.82%  6.61%  3.79%
##     7:   4.83%  2.40%  7.09%  7.06%  5.91%  3.09%
##     8:   4.06%  2.05%  6.71%  5.33%  4.82%  2.50%
##     9:   3.73%  1.85%  5.74%  5.46%  4.48%  2.19%
##    10:   3.24%  1.65%  4.82%  4.66%  4.02%  1.99%
##    11:   2.68%  1.19%  3.82%  4.11%  3.57%  1.62%
##    12:   2.47%  0.96%  3.84%  3.75%  3.28%  1.45%
##    13:   2.24%  0.99%  3.73%  3.25%  2.81%  1.14%
##    14:   1.91%  0.70%  2.96%  2.95%  2.65%  1.03%
##    15:   1.76%  0.63%  2.67%  2.78%  2.33%  1.08%
##    16:   1.60%  0.57%  2.74%  2.25%  2.08%  0.92%
##    17:   1.40%  0.59%  2.08%  2.10%  1.77%  0.94%
##    18:   1.29%  0.48%  2.00%  1.96%  1.84%  0.69%
##    19:   1.17%  0.41%  1.79%  1.81%  1.62%  0.69%
##    20:   1.08%  0.41%  1.55%  1.61%  1.40%  0.80%
##    21:   1.01%  0.39%  1.58%  1.58%  1.27%  0.58%
##    22:   0.93%  0.32%  1.47%  1.29%  1.37%  0.55%
##    23:   0.88%  0.32%  1.19%  1.34%  1.40%  0.53%
##    24:   0.81%  0.27%  1.24%  1.08%  1.34%  0.44%
##    25:   0.82%  0.27%  1.21%  1.17%  1.37%  0.44%
##    26:   0.68%  0.20%  1.00%  0.99%  1.06%  0.47%
##    27:   0.68%  0.14%  1.24%  1.02%  0.96%  0.36%
##    28:   0.66%  0.13%  1.05%  1.05%  1.09%  0.33%
##    29:   0.64%  0.16%  0.95%  1.05%  1.06%  0.30%
##    30:   0.63%  0.16%  0.97%  1.08%  0.96%  0.28%
##    31:   0.61%  0.11%  0.97%  1.08%  0.90%  0.28%
##    32:   0.55%  0.14%  0.76%  1.05%  0.75%  0.28%
##    33:   0.56%  0.13%  0.87%  0.85%  0.96%  0.28%
##    34:   0.57%  0.14%  0.92%  0.94%  0.81%  0.30%
##    35:   0.56%  0.16%  0.84%  0.91%  0.81%  0.33%</code></pre>
<p>I expect the out of sample error to be comparable if the same users and movements are made. However the out of sample error can be substantially bigger for a new athlete.</p>
<p>A special form of cross validation can be done by predicting for an unknown athlete. Then a larger error can be expected if we were to predict the class for a new user, an atlethe we did not train on.</p>
<p>I have simulated this by separating the test into a random sumbsample of 5/6 of the original test set, and a leave one atlethe out sample (in this case Carlitos). The accuracy of the prediction (OOB) dropped initially, but this could be resolved by using more trees, and was 99,5% for the random sample. However the prediction accuracy for the unknown atlethe dropped to around 50%. Remember I trained on 5 atletes and predicted for the 6th one.</p>
<pre class="sourceCode r"><code class="sourceCode r">{userTrain&lt;-trainingCleanNAnzv[trainingCleanNAnzv$user_name!=<span class="st">&quot;carlitos&quot;</span>,]
userTest&lt;-trainingCleanNAnzv[trainingCleanNAnzv$user_name==<span class="st">&quot;carlitos&quot;</span>,]
modFitUser&lt;-<span class="kw">randomForest</span>(userTrain[,<span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">7</span>:<span class="dv">58</span>)],userTrain[,<span class="dv">59</span>],<span class="dt">ntree=</span><span class="dv">45</span>,<span class="dt">do.trace=</span><span class="ot">FALSE</span>)
userPredict&lt;-<span class="kw">predict</span>(modFitUser,<span class="dt">newdata=</span>userTest)
<span class="kw">confusionMatrix</span>(userPredict,userTest$classe)
}</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   A   B   C   D   E
##          A 802 217 276 203 116
##          B  29 402 149 135 295
##          C   0  26  56   9   3
##          D   2  15  12 125  60
##          E   1  30   0  14 135
## 
## Overall Statistics
##                                           
##                Accuracy : 0.4884          
##                  95% CI : (0.4707, 0.5062)
##     No Information Rate : 0.268           
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.3288          
##  Mcnemar's Test P-Value : &lt; 2.2e-16       
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            0.9616   0.5826  0.11359  0.25720  0.22167
## Specificity            0.6435   0.7490  0.98549  0.96611  0.98202
## Pos Pred Value         0.4969   0.3980  0.59574  0.58411  0.75000
## Neg Pred Value         0.9786   0.8630  0.85520  0.87543  0.83834
## Prevalence             0.2680   0.2217  0.15842  0.15617  0.19569
## Detection Rate         0.2577   0.1292  0.01799  0.04017  0.04338
## Detection Prevalence   0.5186   0.3246  0.03021  0.06877  0.05784
## Balanced Accuracy      0.8026   0.6658  0.54954  0.61165  0.60185</code></pre>
<p>Therefore one may conclude it will be difficult to apply this prediction to new ‘unknown’ athletes and the out of sample error for such cases will be much much higher than 0,5%.</p>
</div>



<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
